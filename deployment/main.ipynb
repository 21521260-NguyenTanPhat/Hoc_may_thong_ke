{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "^C\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/ptdat/Desktop/ViSFD/tokenizer/prepare-visfd.py\", line 20, in <module>\n",
      "    visfd[\"lang\"] = visfd.comment.map(detect)\n",
      "                    ^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/ptdat/Desktop/venv/lib/python3.12/site-packages/pandas/core/series.py\", line 4700, in map\n",
      "    new_values = self._map_values(arg, na_action=na_action)\n",
      "                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/ptdat/Desktop/venv/lib/python3.12/site-packages/pandas/core/base.py\", line 921, in _map_values\n",
      "    return algorithms.map_array(arr, mapper, na_action=na_action, convert=convert)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/ptdat/Desktop/venv/lib/python3.12/site-packages/pandas/core/algorithms.py\", line 1743, in map_array\n",
      "    return lib.map_infer(values, mapper, convert=convert)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"lib.pyx\", line 2972, in pandas._libs.lib.map_infer\n",
      "  File \"/home/ptdat/Desktop/venv/lib/python3.12/site-packages/langdetect/detector_factory.py\", line 130, in detect\n",
      "    return detector.detect()\n",
      "           ^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/ptdat/Desktop/venv/lib/python3.12/site-packages/langdetect/detector.py\", line 136, in detect\n",
      "    probabilities = self.get_probabilities()\n",
      "                    ^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/ptdat/Desktop/venv/lib/python3.12/site-packages/langdetect/detector.py\", line 143, in get_probabilities\n",
      "    self._detect_block()\n",
      "  File \"/home/ptdat/Desktop/venv/lib/python3.12/site-packages/langdetect/detector.py\", line 163, in _detect_block\n",
      "    if self._normalize_prob(prob) > self.CONV_THRESHOLD or i >= self.ITERATION_LIMIT:\n",
      "       ^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/ptdat/Desktop/venv/lib/python3.12/site-packages/langdetect/detector.py\", line 226, in _normalize_prob\n",
      "    maxp, sump = 0.0, sum(prob)\n",
      "                      ^^^^^^^^^\n",
      "KeyboardInterrupt\n"
     ]
    }
   ],
   "source": [
    "# Prepare data for tokenizer\n",
    "!python3 tokenizer/prepare-visfd.py \\\n",
    "    ./data/Train.csv \\\n",
    "    --out_dir ./tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2K[00:00:00] Pre-processing files (1 Mo)    ░░░░░░░░░░░░░░░░░░                  1%"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2K[00:00:00] Tokenize words                 ██████████████████ 7688     /     7688[00:00:00] Tokenize words                 ██████████████████ 0        /        0\n",
      "\u001b[2K[00:00:00] Count pairs                    ██████████████████ 7688     /     7688\n",
      "\u001b[2K[00:00:00] Compute merges                 ██████████████████ 3640     /     3640\n"
     ]
    }
   ],
   "source": [
    "# Build tokenizer\n",
    "!python3 tokenizer/build-tokenizer.py \\\n",
    "    ./tokenizer/feedbacks.txt \\\n",
    "    --out_dir ./tokenizer \\\n",
    "    --config_file ./config.json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2024-05-20 09:29:48 INFO  WordSegmenter:24 - Loading Word Segmentation model\n",
      "2024-05-20 09:29:48 INFO  PosTagger:23 - Loading POS Tagging model\n",
      "2024-05-20 09:29:50 INFO  NerRecognizer:34 - Loading NER model\n",
      "2024-05-20 09:29:58 INFO  DependencyParser:32 - Loading Dependency Parsing model\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|\u001b[32m██████████\u001b[0m| 7786/7786 [00:00<00:00, 61182.53it/s]\n",
      "100%|\u001b[32m██████████\u001b[0m| 7786/7786 [00:35<00:00, 219.97it/s]\n",
      "100%|\u001b[32m██████████\u001b[0m| 7786/7786 [00:03<00:00, 2337.50it/s]\n",
      "100%|\u001b[32m██████████\u001b[0m| 1112/1112 [00:00<00:00, 16508.33it/s]\n",
      "100%|\u001b[32m██████████\u001b[0m| 1112/1112 [00:05<00:00, 218.55it/s]\n",
      "100%|\u001b[32m██████████\u001b[0m| 1112/1112 [00:00<00:00, 2470.49it/s]\n",
      "100%|\u001b[32m██████████\u001b[0m| 2224/2224 [00:00<00:00, 59929.79it/s]\n",
      "100%|\u001b[32m██████████\u001b[0m| 2224/2224 [00:10<00:00, 217.08it/s]\n",
      "100%|\u001b[32m██████████\u001b[0m| 2224/2224 [00:01<00:00, 1766.00it/s]\n"
     ]
    }
   ],
   "source": [
    "from modeling.dataset import ViSFDDataset\n",
    "import torch\n",
    "\n",
    "task = \"mtl\"\n",
    "\n",
    "train_set = ViSFDDataset(\"data/Train.csv\", task_type=task)\n",
    "val_set = ViSFDDataset(\"data/Dev.csv\", task_type=task)\n",
    "test_set = ViSFDDataset(\"data/Test.csv\", task_type=task)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "=================================================================\n",
       "Layer (type:depth-idx)                   Param #\n",
       "=================================================================\n",
       "ViSFD_LSTM_CNN                           --\n",
       "├─Embedding: 1-1                         3,072,000\n",
       "├─LSTM: 1-2                              11,550,720\n",
       "├─Conv1d: 1-3                            81,936\n",
       "├─AdaptiveAvgPool1d: 1-4                 --\n",
       "├─AdaptiveMaxPool1d: 1-5                 --\n",
       "├─MTL_ViSFDClassifier: 1-6               --\n",
       "│    └─AspectClassifier: 2-1             --\n",
       "│    │    └─Sequential: 3-1              131,851\n",
       "│    └─PolarityClassifier: 2-2           --\n",
       "│    │    └─ModuleList: 3-2              1,313,310\n",
       "=================================================================\n",
       "Total params: 16,149,817\n",
       "Trainable params: 16,149,817\n",
       "Non-trainable params: 0\n",
       "================================================================="
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from tokenizers import Tokenizer\n",
    "from modeling.modules import ViSFD_LSTM_CNN\n",
    "from modeling.losses import ViSFDLoss\n",
    "\n",
    "tokenizer: Tokenizer = Tokenizer.from_file(\"./tokenizer/tokenizer.json\")\n",
    "model = ViSFD_LSTM_CNN(\n",
    "    tokenizer, \n",
    "    task_type=task, \n",
    "    embed_dim=768,\n",
    "    dropout=0.5,\n",
    "    lstm_hidden_size=512,\n",
    "    lstm_num_layers=2,\n",
    "    cnn_kernel_size=5,\n",
    "    cnn_out_channels=16,\n",
    "    pooling_out_size=64,\n",
    "    output_dropout=0.5,\n",
    "    output_hidden_size=64\n",
    ")\n",
    "# model = ViSFD_LSTM_CNN_v2(\n",
    "#     tokenizer, \n",
    "#     task_type=task, \n",
    "#     embed_dim=768,\n",
    "#     dropout=0.5,\n",
    "#     lstm_hidden_size=512,\n",
    "#     lstm_num_layers=2,\n",
    "#     cnn_kernel_size=5,\n",
    "#     cnn_out_channels=16,\n",
    "#     pooling_out_size=64,\n",
    "#     output_dropout=0.5,\n",
    "#     output_hidden_size=64,\n",
    "#     output_attn_num_heads=8\n",
    "# )\n",
    "loss = ViSFDLoss(\n",
    "    task_type=task, \n",
    "    task_loss_args=dict(\n",
    "        aspect_alpha=1, \n",
    "        aspect_gamma=0,\n",
    "        polarity_weight=1\n",
    "    )\n",
    ")\n",
    "optimizer = torch.optim.AdamW(model.parameters())\n",
    "\n",
    "from torchinfo import summary\n",
    "summary(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from modeling.utils import train\n",
    "from modeling.metrics import AspectF1Score, PolarityF1Score\n",
    "\n",
    "train(\n",
    "    model=model,\n",
    "    loss_fn=loss,\n",
    "    optimizer=optimizer,\n",
    "    train_set=train_set,\n",
    "    validation_set=val_set,\n",
    "    batch_size=64,\n",
    "    epochs=15,\n",
    "    metrics={\n",
    "        \"Aspect F1\": AspectF1Score(task_type=task, aspect_threshold=0.5),\n",
    "        \"Polarity F1\": PolarityF1Score(task_type=task, aspect_threshold=0.5)\n",
    "    },\n",
    "    device=\"cuda\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[1m Evaluation\u001b[0m:   0%|\u001b[34m          \u001b[0m| 0/35 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[1m Evaluation\u001b[0m: 100%|\u001b[34m██████████\u001b[0m| 35/35 [00:01<00:00, 23.75it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.7587565183639526"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from modeling.utils import evaluate\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "evaluate(\n",
    "    model=model,\n",
    "    score_fn=AspectF1Score(\"mtl\", aspect_threshold=0.4),\n",
    "    data=DataLoader(test_set, 64)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from modeling.utils import mtl_decode\n",
    "\n",
    "torch.save((model, mtl_decode), \"model.pt\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
