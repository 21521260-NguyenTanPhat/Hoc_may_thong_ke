{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2024-06-24 21:51:20 INFO  WordSegmenter:24 - Loading Word Segmentation model\n",
      "2024-06-24 21:51:20 INFO  PosTagger:23 - Loading POS Tagging model\n",
      "2024-06-24 21:51:22 INFO  NerRecognizer:34 - Loading NER model\n",
      "2024-06-24 21:51:30 INFO  DependencyParser:32 - Loading Dependency Parsing model\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import regex as re\n",
    "import os\n",
    "from py_vncorenlp import VnCoreNLP\n",
    "from transformers import pipeline\n",
    "\n",
    "cwd = os.getcwd()\n",
    "vncorenlp = VnCoreNLP(save_dir=os.environ[\"VNCORENLP\"])\n",
    "os.chdir(cwd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "uniChars = \"√†√°·∫£√£·∫°√¢·∫ß·∫•·∫©·∫´·∫≠ƒÉ·∫±·∫Ø·∫≥·∫µ·∫∑√®√©·∫ª·∫Ω·∫π√™·ªÅ·∫ø·ªÉ·ªÖ·ªáƒë√¨√≠·ªâƒ©·ªã√≤√≥·ªè√µ·ªç√¥·ªì·ªë·ªï·ªó·ªô∆°·ªù·ªõ·ªü·ª°·ª£√π√∫·ªß≈©·ª•∆∞·ª´·ª©·ª≠·ªØ·ª±·ª≥√Ω·ª∑·ªπ·ªµ√Ä√Å·∫¢√É·∫†√Ç·∫¶·∫§·∫®·∫™·∫¨ƒÇ·∫∞·∫Æ·∫≤·∫¥·∫∂√à√â·∫∫·∫º·∫∏√ä·ªÄ·∫æ·ªÇ·ªÑ·ªÜƒê√å√ç·ªàƒ®·ªä√í√ì·ªé√ï·ªå√î·ªí·ªê·ªî·ªñ·ªò∆†·ªú·ªö·ªû·ª†·ª¢√ô√ö·ª¶≈®·ª§∆Ø·ª™·ª®·ª¨·ªÆ·ª∞·ª≤√ù·ª∂·ª∏·ª¥√ÇƒÇƒê√î∆†∆Ø\"\n",
    "unsignChars = \"aaaaaaaaaaaaaaaaaeeeeeeeeeeediiiiiooooooooooooooooouuuuuuuuuuuyyyyyAAAAAAAAAAAAAAAAAEEEEEEEEEEEDIIIOOOOOOOOOOOOOOOOOOOUUUUUUUUUUUYYYYYAADOOU\"\n",
    "\n",
    "\n",
    "def loaddicchar():\n",
    "    dic = {}\n",
    "    char1252 = '√†|√°|·∫£|√£|·∫°|·∫ß|·∫•|·∫©|·∫´|·∫≠|·∫±|·∫Ø|·∫≥|·∫µ|·∫∑|√®|√©|·∫ª|·∫Ω|·∫π|·ªÅ|·∫ø|·ªÉ|·ªÖ|·ªá|√¨|√≠|·ªâ|ƒ©|·ªã|√≤|√≥|·ªè|√µ|·ªç|·ªì|·ªë|·ªï|·ªó|·ªô|·ªù|·ªõ|·ªü|·ª°|·ª£|√π|√∫|·ªß|≈©|·ª•|·ª´|·ª©|·ª≠|·ªØ|·ª±|·ª≥|√Ω|·ª∑|·ªπ|·ªµ|√Ä|√Å|·∫¢|√É|·∫†|·∫¶|·∫§|·∫®|·∫™|·∫¨|·∫∞|·∫Æ|·∫≤|·∫¥|·∫∂|√à|√â|·∫∫|·∫º|·∫∏|·ªÄ|·∫æ|·ªÇ|·ªÑ|·ªÜ|√å|√ç|·ªà|ƒ®|·ªä|√í|√ì|·ªé|√ï|·ªå|·ªí|·ªê|·ªî|·ªñ|·ªò|·ªú|·ªö|·ªû|·ª†|·ª¢|√ô|√ö|·ª¶|≈®|·ª§|·ª™|·ª®|·ª¨|·ªÆ|·ª∞|·ª≤|√ù|·ª∂|·ª∏|·ª¥'.split(\n",
    "        '|')\n",
    "    charutf8 = \"√†|√°|·∫£|√£|·∫°|·∫ß|·∫•|·∫©|·∫´|·∫≠|·∫±|·∫Ø|·∫≥|·∫µ|·∫∑|√®|√©|·∫ª|·∫Ω|·∫π|·ªÅ|·∫ø|·ªÉ|·ªÖ|·ªá|√¨|√≠|·ªâ|ƒ©|·ªã|√≤|√≥|·ªè|√µ|·ªç|·ªì|·ªë|·ªï|·ªó|·ªô|·ªù|·ªõ|·ªü|·ª°|·ª£|√π|√∫|·ªß|≈©|·ª•|·ª´|·ª©|·ª≠|·ªØ|·ª±|·ª≥|√Ω|·ª∑|·ªπ|·ªµ|√Ä|√Å|·∫¢|√É|·∫†|·∫¶|·∫§|·∫®|·∫™|·∫¨|·∫∞|·∫Æ|·∫≤|·∫¥|·∫∂|√à|√â|·∫∫|·∫º|·∫∏|·ªÄ|·∫æ|·ªÇ|·ªÑ|·ªÜ|√å|√ç|·ªà|ƒ®|·ªä|√í|√ì|·ªé|√ï|·ªå|·ªí|·ªê|·ªî|·ªñ|·ªò|·ªú|·ªö|·ªû|·ª†|·ª¢|√ô|√ö|·ª¶|≈®|·ª§|·ª™|·ª®|·ª¨|·ªÆ|·ª∞|·ª≤|√ù|·ª∂|·ª∏|·ª¥\".split(\n",
    "        '|')\n",
    "    for i in range(len(char1252)):\n",
    "        dic[char1252[i]] = charutf8[i]\n",
    "    return dic\n",
    "\n",
    "\n",
    "dicchar = loaddicchar()\n",
    "\n",
    "\n",
    "def convert_unicode(txt):\n",
    "    return re.sub(\n",
    "        r'√†|√°|·∫£|√£|·∫°|·∫ß|·∫•|·∫©|·∫´|·∫≠|·∫±|·∫Ø|·∫≥|·∫µ|·∫∑|√®|√©|·∫ª|·∫Ω|·∫π|·ªÅ|·∫ø|·ªÉ|·ªÖ|·ªá|√¨|√≠|·ªâ|ƒ©|·ªã|√≤|√≥|·ªè|√µ|·ªç|·ªì|·ªë|·ªï|·ªó|·ªô|·ªù|·ªõ|·ªü|·ª°|·ª£|√π|√∫|·ªß|≈©|·ª•|·ª´|·ª©|·ª≠|·ªØ|·ª±|·ª≥|√Ω|·ª∑|·ªπ|·ªµ|√Ä|√Å|·∫¢|√É|·∫†|·∫¶|·∫§|·∫®|·∫™|·∫¨|·∫∞|·∫Æ|·∫≤|·∫¥|·∫∂|√à|√â|·∫∫|·∫º|·∫∏|·ªÄ|·∫æ|·ªÇ|·ªÑ|·ªÜ|√å|√ç|·ªà|ƒ®|·ªä|√í|√ì|·ªé|√ï|·ªå|·ªí|·ªê|·ªî|·ªñ|·ªò|·ªú|·ªö|·ªû|·ª†|·ª¢|√ô|√ö|·ª¶|≈®|·ª§|·ª™|·ª®|·ª¨|·ªÆ|·ª∞|·ª≤|√ù|·ª∂|·ª∏|·ª¥',\n",
    "        lambda x: dicchar[x.group()], txt\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "bang_nguyen_am = [['a', '√†', '√°', '·∫£', '√£', '·∫°', 'a'],\n",
    "                  ['ƒÉ', '·∫±', '·∫Ø', '·∫≥', '·∫µ', '·∫∑', 'aw'],\n",
    "                  ['√¢', '·∫ß', '·∫•', '·∫©', '·∫´', '·∫≠', 'aa'],\n",
    "                  ['e', '√®', '√©', '·∫ª', '·∫Ω', '·∫π', 'e'],\n",
    "                  ['√™', '·ªÅ', '·∫ø', '·ªÉ', '·ªÖ', '·ªá', 'ee'],\n",
    "                  ['i', '√¨', '√≠', '·ªâ', 'ƒ©', '·ªã', 'i'],\n",
    "                  ['o', '√≤', '√≥', '·ªè', '√µ', '·ªç', 'o'],\n",
    "                  ['√¥', '·ªì', '·ªë', '·ªï', '·ªó', '·ªô', 'oo'],\n",
    "                  ['∆°', '·ªù', '·ªõ', '·ªü', '·ª°', '·ª£', 'ow'],\n",
    "                  ['u', '√π', '√∫', '·ªß', '≈©', '·ª•', 'u'],\n",
    "                  ['∆∞', '·ª´', '·ª©', '·ª≠', '·ªØ', '·ª±', 'uw'],\n",
    "                  ['y', '·ª≥', '√Ω', '·ª∑', '·ªπ', '·ªµ', 'y']]\n",
    "bang_ky_tu_dau = ['', 'f', 's', 'r', 'x', 'j']\n",
    "\n",
    "nguyen_am_to_ids = {}\n",
    "\n",
    "for i in range(len(bang_nguyen_am)):\n",
    "    for j in range(len(bang_nguyen_am[i]) - 1):\n",
    "        nguyen_am_to_ids[bang_nguyen_am[i][j]] = (i, j)\n",
    "\n",
    "def chuan_hoa_dau_tu_tieng_viet(word):\n",
    "    if not is_valid_vietnam_word(word):\n",
    "        return word\n",
    "\n",
    "    chars = list(word)\n",
    "    dau_cau = 0\n",
    "    nguyen_am_index = []\n",
    "    qu_or_gi = False\n",
    "    for index, char in enumerate(chars):\n",
    "        x, y = nguyen_am_to_ids.get(char, (-1, -1))\n",
    "        if x == -1:\n",
    "            continue\n",
    "        elif x == 9:  # check qu\n",
    "            if index != 0 and chars[index - 1] == 'q':\n",
    "                chars[index] = 'u'\n",
    "                qu_or_gi = True\n",
    "        elif x == 5:  # check gi\n",
    "            if index != 0 and chars[index - 1] == 'g':\n",
    "                chars[index] = 'i'\n",
    "                qu_or_gi = True\n",
    "        if y != 0:\n",
    "            dau_cau = y\n",
    "            chars[index] = bang_nguyen_am[x][0]\n",
    "        if not qu_or_gi or index != 1:\n",
    "            nguyen_am_index.append(index)\n",
    "    if len(nguyen_am_index) < 2:\n",
    "        if qu_or_gi:\n",
    "            if len(chars) == 2:\n",
    "                x, y = nguyen_am_to_ids.get(chars[1])\n",
    "                chars[1] = bang_nguyen_am[x][dau_cau]\n",
    "            else:\n",
    "                x, y = nguyen_am_to_ids.get(chars[2], (-1, -1))\n",
    "                if x != -1:\n",
    "                    chars[2] = bang_nguyen_am[x][dau_cau]\n",
    "                else:\n",
    "                    chars[1] = bang_nguyen_am[5][dau_cau] if chars[1] == 'i' else bang_nguyen_am[9][dau_cau]\n",
    "            return ''.join(chars)\n",
    "        return word\n",
    "\n",
    "    for index in nguyen_am_index:\n",
    "        x, y = nguyen_am_to_ids[chars[index]]\n",
    "        if x == 4 or x == 8:  # √™, ∆°\n",
    "            chars[index] = bang_nguyen_am[x][dau_cau]\n",
    "            # for index2 in nguyen_am_index:\n",
    "            #     if index2 != index:\n",
    "            #         x, y = nguyen_am_to_ids[chars[index]]\n",
    "            #         chars[index2] = bang_nguyen_am[x][0]\n",
    "            return ''.join(chars)\n",
    "\n",
    "    if len(nguyen_am_index) == 2:\n",
    "        if nguyen_am_index[-1] == len(chars) - 1:\n",
    "            x, y = nguyen_am_to_ids[chars[nguyen_am_index[0]]]\n",
    "            chars[nguyen_am_index[0]] = bang_nguyen_am[x][dau_cau]\n",
    "            # x, y = nguyen_am_to_ids[chars[nguyen_am_index[1]]]\n",
    "            # chars[nguyen_am_index[1]] = bang_nguyen_am[x][0]\n",
    "        else:\n",
    "            # x, y = nguyen_am_to_ids[chars[nguyen_am_index[0]]]\n",
    "            # chars[nguyen_am_index[0]] = bang_nguyen_am[x][0]\n",
    "            x, y = nguyen_am_to_ids[chars[nguyen_am_index[1]]]\n",
    "            chars[nguyen_am_index[1]] = bang_nguyen_am[x][dau_cau]\n",
    "    else:\n",
    "        # x, y = nguyen_am_to_ids[chars[nguyen_am_index[0]]]\n",
    "        # chars[nguyen_am_index[0]] = bang_nguyen_am[x][0]\n",
    "        x, y = nguyen_am_to_ids[chars[nguyen_am_index[1]]]\n",
    "        chars[nguyen_am_index[1]] = bang_nguyen_am[x][dau_cau]\n",
    "        # x, y = nguyen_am_to_ids[chars[nguyen_am_index[2]]]\n",
    "        # chars[nguyen_am_index[2]] = bang_nguyen_am[x][0]\n",
    "    return ''.join(chars)\n",
    "\n",
    "\n",
    "def is_valid_vietnam_word(word):\n",
    "    chars = list(word)\n",
    "    nguyen_am_index = -1\n",
    "    for index, char in enumerate(chars):\n",
    "        x, y = nguyen_am_to_ids.get(char, (-1, -1))\n",
    "        if x != -1:\n",
    "            if nguyen_am_index == -1:\n",
    "                nguyen_am_index = index\n",
    "            else:\n",
    "                if index - nguyen_am_index != 1:\n",
    "                    return False\n",
    "                nguyen_am_index = index\n",
    "    return True\n",
    "\n",
    "\n",
    "def chuan_hoa_dau_cau_tieng_viet(sentence):\n",
    "    \"\"\"\n",
    "        Chuy·ªÉn c√¢u ti·∫øng vi·ªát v·ªÅ chu·∫©n g√µ d·∫•u ki·ªÉu c≈©.\n",
    "        :param sentence:\n",
    "        :return:\n",
    "        \"\"\"\n",
    "    words = sentence.split()\n",
    "    for index, word in enumerate(words):\n",
    "        cw = re.sub(r'(^\\p{P}*)([p{L}.]*\\p{L}+)(\\p{P}*$)', r'\\1/\\2/\\3', word).split('/')\n",
    "        # print(cw)\n",
    "        if len(cw) == 3:\n",
    "            cw[1] = chuan_hoa_dau_tu_tieng_viet(cw[1])\n",
    "        words[index] = ''.join(cw)\n",
    "    return ' '.join(words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "sp_word_sub = {\n",
    "    \"@@\": \"confuseeyes\",\n",
    "    \"‚ÑÖ\": \"%\",\n",
    "    r\"/\": \" fraction \",\n",
    "    r\":\\)+\": \"smileface\",\n",
    "    r\";\\)+\": \"smileface\",\n",
    "    r\":\\*+\": \"kissingface\",\n",
    "    r\"=\\)+\": \"playfulsmileface\",\n",
    "    r\"=\\(+\": \"playfulsadface\",\n",
    "    r\":\\(+\": \"sadface\",\n",
    "    r\":3+\": \"threeface\",\n",
    "    r\":v+\": \"vface\",\n",
    "    r\"\\^\\^\": \"kindsmile\",\n",
    "    r\"\\^_\\^\": \"kindmountsmile\",\n",
    "    r\"\\^\\.\\^\": \"kindmountsmile\",\n",
    "    r\"-_-\": \"disapointface\",\n",
    "    r\"\\._\\.\": \"confusedface\",\n",
    "    r\":>+\": \"cutesmile\",\n",
    "    r\"(\\|)w(\\|)\": \"fancycryface\",\n",
    "    r\":\\|\": \"mutedface\",\n",
    "    r\":d+\": \"laughface\",\n",
    "    r\"<3\": \"loveicon\",\n",
    "    r\"\\.{2,}\": \"threedot\",\n",
    "    r\"-{1,}>{1,}\": \"arrow\",\n",
    "    r\"={1,}>{1,}\": \"arrow\",\n",
    "    r\"(\\d+)h\": r\"\\1 gi·ªù\",\n",
    "    r\"(\\d+)'\": r\"\\1 ph√∫t\",\n",
    "    r\"(\\d+)trieu\": r\"\\1 tri·ªáu\",\n",
    "    r\"(\\d+)\\s?tr\": r\"\\1 tri·ªáu\",\n",
    "    r\"blut\\w+\": \"bluetooth\",\n",
    "    r\"(\\d+)\\s\\*\": r\"\\1 sao\"\n",
    "}\n",
    "\n",
    "replace_dict = {\n",
    "    \"/\": \"fraction\",\n",
    "    \"wf\": \"wifi\",\n",
    "    \"wifj\": \"wifi\",\n",
    "    \"wjfj\": \"wifi\",\n",
    "    \"wjfi\": \"wifi\",\n",
    "    \"wiffi\": \"wifi\",\n",
    "    \"wj\": \"wifi\",\n",
    "    \"ko\": \"kh√¥ng\",\n",
    "    \"k\": \"kh√¥ng\",\n",
    "    \"hong\": \"kh√¥ng\",\n",
    "    \"ƒëc\": \"ƒë∆∞·ª£c\",\n",
    "    \"sp\": \"s·∫£n ph·∫©m\",\n",
    "    \"fb\": \"facebook\",\n",
    "    \"ytb\": \"youtube\",\n",
    "    \"yt\": \"youtube\",\n",
    "    \"mes\": \"messenger\",\n",
    "    \"mess\": \"messenger\",\n",
    "    \"tgdƒë\": \"thegioididong\",\n",
    "    \"nv\": \"nh√¢n vi√™n\",\n",
    "    \"ss\": \"samsung\",\n",
    "    \"ip\": \"iphone\",\n",
    "    \"appel\": \"apple\",\n",
    "    \"oke\": \"ok\",\n",
    "    \"okie\": \"ok\",\n",
    "    \"okey\": \"ok\",\n",
    "    \"oki\": \"ok\",\n",
    "    \"oce\": \"ok\",\n",
    "    \"okela\": \"ok\",\n",
    "    \"mk\": \"m√¨nh\",\n",
    "    \"sd\": \"s·ª≠ d·ª•ng\",\n",
    "    \"sdung\": \"s·ª≠ d·ª•ng\",\n",
    "    \"ae\": \"anh em\",\n",
    "    \"lq\": \"li√™n qu√¢n\",\n",
    "    \"lqmb\": \"li√™n qu√¢n mobile\",\n",
    "    \"lun\": \"lu√¥n\",\n",
    "    \"ng\": \"ng∆∞·ªùi\",\n",
    "    \"ad\": \"admin\",\n",
    "    \"ms\": \"m·ªõi\",\n",
    "    \"cx\": \"c≈©ng\",\n",
    "    \"c≈©g\": \"c≈©ng\",\n",
    "    \"nh√¨u\": \"nhi·ªÅu\",\n",
    "    \"bth\": \"b√¨nh th∆∞·ªùng\",\n",
    "    \"bthg\": \"b√¨nh th∆∞·ªùng\",\n",
    "    \"ngta\": \"ng∆∞·ªùi ta\",\n",
    "    \"dow\": \"download\",\n",
    "    \"hdh\": \"h·ªá ƒëi·ªÅu h√†nh\",\n",
    "    \"hƒëh\": \"h·ªá ƒëi·ªÅu h√†nh\",\n",
    "    \"cammera\": \"camera\",\n",
    "    \"dt\": \"ƒëi·ªán tho·∫°i\",\n",
    "    \"dthoai\": \"ƒëi·ªán tho·∫°i\",\n",
    "    \"dth\": \"ƒëi·ªán tho·∫°i\",\n",
    "    \"ƒëth\": \"ƒëi·ªán tho·∫°i\",\n",
    "    \"hk\": \"kh√¥ng\",\n",
    "    \"j\": \"g√¨\",\n",
    "    \"ji\": \"g√¨\",\n",
    "    \"mn\": \"m·ªçi ng∆∞·ªùi\",\n",
    "    \"m.n\": \"m·ªçi ng∆∞·ªùi\",\n",
    "    \"mjh\": \"m√¨nh\",\n",
    "    \"mjk\": \"m√¨nh\",\n",
    "    \"l·∫Øc\": \"lag\",\n",
    "    \"l√°c\": \"lag\",\n",
    "    \"lang\": \"lag\",\n",
    "    \"nhah\": \"nhanh\",\n",
    "    \"n√≥ichung\": \"n√≥i chung\",\n",
    "    \"zl\": \"zalo\",\n",
    "    \"s√≥g\": \"s√≥ng\",\n",
    "    \"r·∫Ω\": \"r·∫ª\",\n",
    "    \"trc\": \"tr∆∞·ªõc\",\n",
    "    \"ch√≠p\": \"chip\",\n",
    "    \"bin\": \"pin\",\n",
    "    \"lm\": \"l√†m\",\n",
    "    \"bik\": \"bi·∫øt\",\n",
    "    \"hog\": \"kh√¥ng\",\n",
    "    \"z·ªèm\": \"d·ªïm\",\n",
    "    \"z\": \"v·∫≠y\",\n",
    "    \"v\": \"v·∫≠y\",\n",
    "    \"nhah\": \"nhanh\",\n",
    "    \"r\": \"r·ªìi\",\n",
    "    \"·ªón\": \"·ªïn\",\n",
    "    \"nh√¨u\": \"nhi·ªÅu\",\n",
    "    \"w√°\": \"qu√°\",\n",
    "    \"wep\": \"web\",\n",
    "    \"wed\": \"web\",\n",
    "    \"fim\": \"phim\",\n",
    "    \"film\": \"phim\",\n",
    "    \"x·∫°c\": \"s·∫°c\",\n",
    "    \"x√†i\": \"s√†i\",\n",
    "    \"het\": \"h·∫øt\",\n",
    "    \"lun\": \"lu√¥n\",\n",
    "    \"e\": \"em\",\n",
    "    \"a\": \"anh\",\n",
    "    \"bjo\": \"b√¢y gi·ªù\",\n",
    "    \"vl\": \"v√£i l·ªìn\",\n",
    "    \"sac\": \"s·∫°c\",\n",
    "    \"vidieo\": \"video\",\n",
    "    \"t√©t\": \"test\",\n",
    "    \"tes\": \"test\",\n",
    "    \"thik\": \"th√≠ch\",\n",
    "    \"fai\": \"ph·∫£i\",\n",
    "    \"‚úã\": \"tay\",\n",
    "    \"üîã\": \"pin\",\n",
    "    \"‚òÜ\": \"sao\",\n",
    "    \"supper\": \"super\",\n",
    "    \"l·ªïi\": \"l·ªói\",\n",
    "    \"lo√°t\": \"load\",\n",
    "    \"thui\": \"th√¥i\",\n",
    "    \"r√πi\": \"r·ªìi\",\n",
    "    \"·ªón\": \"·ªïn\",\n",
    "    \"l·ªïi\": \"l·ªói\",\n",
    "    \"su·ªëng\": \"xu·ªëng\",\n",
    "    \"selfi\": \"selfie\",\n",
    "    \"gg\": \"google\",\n",
    "    \"cam on\": \"c·∫£m ∆°n\",\n",
    "    \"tg\": \"th·ªùi gian\",\n",
    "    \"nchung\": \"n√≥i chung\",\n",
    "    \"‚ù§\": \"loveicon\",\n",
    "    \"tr·∫°i nghi·ªám\": \"tr·∫£i nghi·ªám\",\n",
    "    \"d·∫•t\": \"r·∫•t\",\n",
    "    \"ƒë·ª©g\": \"ƒë·ª©ng\",\n",
    "    \"b·∫±g\": \"b·∫±ng\",\n",
    "    \"m√¨h\": \"m√¨nh\",\n",
    "    \"ƒëag\": \"ƒëang\",\n",
    "    \"thoi\": \"th√¥i\",\n",
    "    \"c·ªßng\": \"c≈©ng\",\n",
    "    \"ƒë·∫£\": \"ƒë√£\",\n",
    "    \"m√†ng\": \"m√†n\",\n",
    "    \"ff\": \"free fire\",\n",
    "    \"cod\": \"call of duty\",\n",
    "    \"moi th·ª©\": \"m·ªçi th·ª©\",\n",
    "    \"moi thu\": \"m·ªçi th·ª©\",\n",
    "    \"moi th∆∞\": \"m·ªçi th·ª©\",\n",
    "    \"moi ng∆∞·ªùi\": \"m·ªçi ng∆∞·ªùi\",\n",
    "    \"moi\": \"m·ªõi\",\n",
    "    \"dk\": \"ƒë∆∞·ª£c\",\n",
    "    \"ƒëk\": \"ƒë∆∞·ª£c\",\n",
    "    \"nh·∫≠y\": \"nh·∫°y\",\n",
    "    \"ak\": \"√°\",\n",
    "    \"ghe\": \"nghe\",\n",
    "    \"b√πn\": \"bu·ªìn\",\n",
    "    \"bit\": \"bi·∫øt\",\n",
    "    \"b√≠t\": \"bi·∫øt\",\n",
    "    \"bnhieu\": \"bao nhi√™u\",\n",
    "    \"d·ª•g\": \"d·ª•ng\",\n",
    "    \"tk\": \"t√†i kho·∫£n\",\n",
    "    \"sƒÖc\": \"s·∫°c\",\n",
    "    \"r√¢t\": \"r√¢t\",\n",
    "    \"haz\": \"haiz\",\n",
    "    \"sai l√†m\": \"sai l·∫ßm\",\n",
    "    \"flim\": \"phim\",\n",
    "    \"x∆∞·ªõt\": \"x∆∞·ªõc\",\n",
    "    \"vi·ªÅng\": \"vi·ªÅn\"\n",
    "}\n",
    "\n",
    "drop_indices = [1537, 2754, 4267, 6536]\n",
    "\n",
    "def normalize(text: str, track_change=False):\n",
    "    # Lowercase\n",
    "    text = text.lower()\n",
    "\n",
    "    text = re.sub(r\"((https?|ftp|file):\\/{2,3})+([-\\w+&@#/%=~|$?!:,.]*)|(www.)+([-\\w+&@#/%=~|$?!:,.]*)\", \"urllink\", text)\n",
    "\n",
    "    # Remove dup trailing chars (troiiiii -> troi)\n",
    "    text = re.sub(r\"([\\D\\w])\\1+\\b\", r\"\\1\", text)\n",
    "    if track_change:\n",
    "        print(\"Dedup trailing: \", text)\n",
    "\n",
    "    # Replace special symbol to word\n",
    "    for pttn, repl in sp_word_sub.items():\n",
    "        text = re.sub(fr\"{pttn}\", f\" {repl} \", text)\n",
    "    if track_change:\n",
    "        print(\"Replace special word: \", text)\n",
    "    \n",
    "    # Correct misspelled word\n",
    "    def replace(match):\n",
    "        orig = match.group(1)\n",
    "        word = \" \" + replace_dict.get(orig, orig) + \" \"\n",
    "        return word\n",
    "    text = re.sub(r\"\\b(\\S+)\\b\", replace, text)\n",
    "    if track_change:\n",
    "        print(\"Correct misspelled word: \", text)\n",
    "\n",
    "    # Normalize string encoding\n",
    "    text = convert_unicode(text)\n",
    "    if track_change:\n",
    "        print(\"Normalize string encoding: \", text)\n",
    "\n",
    "    # Vietnamese unicode normalization\n",
    "    text = chuan_hoa_dau_cau_tieng_viet(text)\n",
    "    if track_change:\n",
    "        print(\"Vietnamese unicode normalization: \", text)\n",
    "\n",
    "    # Eliminate decimal delimiter (9.000 -> 9000)\n",
    "    text = re.sub(r\"(?<=\\d)\\.(?=\\d{3})\", \"\", text)\n",
    "    if track_change:\n",
    "        print(\"Eliminate decimal delimiter: \", text)\n",
    "    \n",
    "    # Split between value and unit (300km -> 300 km)\n",
    "    text = re.sub(r\"(\\d+)(\\D+)\", r\"\\1 \\2\", text)\n",
    "    if track_change:\n",
    "        print(\"Split between value and unit: \", text)\n",
    "\n",
    "    # Split by punctuations\n",
    "    text = \" \".join(\n",
    "        re.split(\"([\"+re.escape(\"!\\\"#$%&\\'()*+,-./:;<=>?@[\\\\]^`{|}~\")+\"])\", text)\n",
    "    )\n",
    "    if track_change:\n",
    "        print(\"Split by punctuations: \", text)\n",
    "\n",
    "    # Split by emoticons\n",
    "    text = \" \".join(\n",
    "        re.split(\"([\"\n",
    "            u\"\\U0001F600-\\U0001F64F\"  # emoticons\n",
    "            u\"\\U0001F300-\\U0001F5FF\"  # symbols & pictographs\n",
    "            u\"\\U0001F680-\\U0001F6FF\"  # transport & map symbols\n",
    "            u\"\\U0001F1E0-\\U0001F1FF\"  # flags (iOS)\n",
    "            u\"\\U00002702-\\U000027B0\"\n",
    "            u\"\\U000024C2-\\U0001F251\"\n",
    "            u\"\\U0001f926-\\U0001f937\"\n",
    "            u'\\U00010000-\\U0010ffff'\n",
    "            u\"\\u200d\"\n",
    "            u\"\\u2640-\\u2642\"\n",
    "            u\"\\u2600-\\u2B55\"\n",
    "            u\"\\u23cf\"\n",
    "            u\"\\u23e9\"\n",
    "            u\"\\u231a\"\n",
    "            u\"\\u3030\"\n",
    "            u\"\\ufe0f\"\n",
    "            u\"\\u221a\"\n",
    "        \"])\", text)\n",
    "    )\n",
    "\n",
    "    # Word segmentation\n",
    "    # text = \" \".join(vncorenlp.word_segment(text))\n",
    "    \n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dedup trailing:  ·ªßa cho em h·ªèi l√† iphone 7plus ch√≠nh h√£ng c·ªßa tgd l√† kh√¥ng c√≥ lo·∫°i 64g hay 128g d·∫° ? hay l√† n√≥ ƒë·ªôc quy·ªÅn 32g √† . c√≤n m·∫•y c√°i 7plus g cao l√† h√†ng nh√°i hay sao ? t·∫°i em ra tgd ki·∫øm m√† ch·ªâ c√≥ 32g th√¥i n√™n mua lu√¥n üòÇ gi·ªù th·∫Øc m·∫Øc l√™n h·ªèi ü§îü§î\n",
      "Replace special word:  ·ªßa cho em h·ªèi l√† iphone 7plus ch√≠nh h√£ng c·ªßa tgd l√† kh√¥ng c√≥ lo·∫°i 64g hay 128g d·∫° ? hay l√† n√≥ ƒë·ªôc quy·ªÅn 32g √† . c√≤n m·∫•y c√°i 7plus g cao l√† h√†ng nh√°i hay sao ? t·∫°i em ra tgd ki·∫øm m√† ch·ªâ c√≥ 32g th√¥i n√™n mua lu√¥n üòÇ gi·ªù th·∫Øc m·∫Øc l√™n h·ªèi ü§îü§î\n",
      "Correct misspelled word:   ·ªßa   cho   em   h·ªèi   l√†   iphone   7plus   ch√≠nh   h√£ng   c·ªßa   tgd   l√†   kh√¥ng   c√≥   lo·∫°i   64g   hay   128g   d·∫°  ?  hay   l√†   n√≥   ƒë·ªôc   quy·ªÅn   32g   √†  .  c√≤n   m·∫•y   c√°i   7plus   g   cao   l√†   h√†ng   nh√°i   hay   sao  ?  t·∫°i   em   ra   tgd   ki·∫øm   m√†   ch·ªâ   c√≥   32g   th√¥i   n√™n   mua   lu√¥n  üòÇ  gi·ªù   th·∫Øc   m·∫Øc   l√™n   h·ªèi  ü§îü§î\n",
      "Normalize string encoding:   ·ªßa   cho   em   h·ªèi   l√†   iphone   7plus   ch√≠nh   h√£ng   c·ªßa   tgd   l√†   kh√¥ng   c√≥   lo·∫°i   64g   hay   128g   d·∫°  ?  hay   l√†   n√≥   ƒë·ªôc   quy·ªÅn   32g   √†  .  c√≤n   m·∫•y   c√°i   7plus   g   cao   l√†   h√†ng   nh√°i   hay   sao  ?  t·∫°i   em   ra   tgd   ki·∫øm   m√†   ch·ªâ   c√≥   32g   th√¥i   n√™n   mua   lu√¥n  üòÇ  gi·ªù   th·∫Øc   m·∫Øc   l√™n   h·ªèi  ü§îü§î\n",
      "Vietnamese unicode normalization:  ·ªßa cho em h·ªèi l√† iphone 7plus ch√≠nh h√£ng c·ªßa tgd l√† kh√¥ng c√≥ lo·∫°i 64g hay 128g d·∫° ? hay l√† n√≥ ƒë·ªôc quy·ªÅn 32g √† . c√≤n m·∫•y c√°i 7plus g cao l√† h√†ng nh√°i hay sao ? t·∫°i em ra tgd ki·∫øm m√† ch·ªâ c√≥ 32g th√¥i n√™n mua lu√¥n üòÇ gi·ªù th·∫Øc m·∫Øc l√™n h·ªèi ü§îü§î\n",
      "Eliminate decimal delimiter:  ·ªßa cho em h·ªèi l√† iphone 7plus ch√≠nh h√£ng c·ªßa tgd l√† kh√¥ng c√≥ lo·∫°i 64g hay 128g d·∫° ? hay l√† n√≥ ƒë·ªôc quy·ªÅn 32g √† . c√≤n m·∫•y c√°i 7plus g cao l√† h√†ng nh√°i hay sao ? t·∫°i em ra tgd ki·∫øm m√† ch·ªâ c√≥ 32g th√¥i n√™n mua lu√¥n üòÇ gi·ªù th·∫Øc m·∫Øc l√™n h·ªèi ü§îü§î\n",
      "Split between value and unit:  ·ªßa cho em h·ªèi l√† iphone 7 plus ch√≠nh h√£ng c·ªßa tgd l√† kh√¥ng c√≥ lo·∫°i 64 g hay 128 g d·∫° ? hay l√† n√≥ ƒë·ªôc quy·ªÅn 32 g √† . c√≤n m·∫•y c√°i 7 plus g cao l√† h√†ng nh√°i hay sao ? t·∫°i em ra tgd ki·∫øm m√† ch·ªâ c√≥ 32 g th√¥i n√™n mua lu√¥n üòÇ gi·ªù th·∫Øc m·∫Øc l√™n h·ªèi ü§îü§î\n",
      "Split by punctuations:  ·ªßa cho em h·ªèi l√† iphone 7 plus ch√≠nh h√£ng c·ªßa tgd l√† kh√¥ng c√≥ lo·∫°i 64 g hay 128 g d·∫°  ?  hay l√† n√≥ ƒë·ªôc quy·ªÅn 32 g √†  .  c√≤n m·∫•y c√°i 7 plus g cao l√† h√†ng nh√°i hay sao  ?  t·∫°i em ra tgd ki·∫øm m√† ch·ªâ c√≥ 32 g th√¥i n√™n mua lu√¥n üòÇ gi·ªù th·∫Øc m·∫Øc l√™n h·ªèi ü§îü§î\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'·ªßa cho em h·ªèi l√† iphone 7 plus ch√≠nh h√£ng c·ªßa tgd l√† kh√¥ng c√≥ lo·∫°i 64 g hay 128 g d·∫°  ?  hay l√† n√≥ ƒë·ªôc quy·ªÅn 32 g √†  .  c√≤n m·∫•y c√°i 7 plus g cao l√† h√†ng nh√°i hay sao  ?  t·∫°i em ra tgd ki·∫øm m√† ch·ªâ c√≥ 32 g th√¥i n√™n mua lu√¥n  üòÇ  gi·ªù th·∫Øc m·∫Øc l√™n h·ªèi  ü§î  ü§î '"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "normalize(\"·ª¶a cho em h·ªèi l√† iphone 7plus ch√≠nh h√£ng c·ªßa tgdd l√† kh√¥ng c√≥ lo·∫°i 64G hay 128G d·∫° ? Hay l√† n√≥ ƒë·ªôc quy·ªÅn 32G √† . C√≤n m·∫•y c√°i 7plus G cao l√† h√†ng nh√°i hay sao ? T·∫°i em ra tgdd ki·∫øm m√† ch·ªâ c√≥ 32G th√¥i n√™n mua lu√¥n üòÇ gi·ªù th·∫Øc m·∫Øc l√™n h·ªèi ü§îü§î\", track_change=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "\n",
    "for dset in [\"Train\", \"Dev\", \"Test\"]:\n",
    "    df = pd.read_csv(f\"data/{dset}.csv\")\n",
    "    if dset == \"Train\":\n",
    "        df.drop(index=drop_indices)\n",
    "    normalized = df.comment.map(normalize)\n",
    "    with open(f\"data/prep-{dset}.txt\", \"w\") as f:\n",
    "        f.write(\"\\n\".join(normalized))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
