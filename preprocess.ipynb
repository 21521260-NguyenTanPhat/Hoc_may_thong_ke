{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2024-06-24 21:51:20 INFO  WordSegmenter:24 - Loading Word Segmentation model\n",
      "2024-06-24 21:51:20 INFO  PosTagger:23 - Loading POS Tagging model\n",
      "2024-06-24 21:51:22 INFO  NerRecognizer:34 - Loading NER model\n",
      "2024-06-24 21:51:30 INFO  DependencyParser:32 - Loading Dependency Parsing model\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import regex as re\n",
    "import os\n",
    "from py_vncorenlp import VnCoreNLP\n",
    "from transformers import pipeline\n",
    "\n",
    "cwd = os.getcwd()\n",
    "vncorenlp = VnCoreNLP(save_dir=os.environ[\"VNCORENLP\"])\n",
    "os.chdir(cwd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "uniChars = \"àáảãạâầấẩẫậăằắẳẵặèéẻẽẹêềếểễệđìíỉĩịòóỏõọôồốổỗộơờớởỡợùúủũụưừứửữựỳýỷỹỵÀÁẢÃẠÂẦẤẨẪẬĂẰẮẲẴẶÈÉẺẼẸÊỀẾỂỄỆĐÌÍỈĨỊÒÓỎÕỌÔỒỐỔỖỘƠỜỚỞỠỢÙÚỦŨỤƯỪỨỬỮỰỲÝỶỸỴÂĂĐÔƠƯ\"\n",
    "unsignChars = \"aaaaaaaaaaaaaaaaaeeeeeeeeeeediiiiiooooooooooooooooouuuuuuuuuuuyyyyyAAAAAAAAAAAAAAAAAEEEEEEEEEEEDIIIOOOOOOOOOOOOOOOOOOOUUUUUUUUUUUYYYYYAADOOU\"\n",
    "\n",
    "\n",
    "def loaddicchar():\n",
    "    dic = {}\n",
    "    char1252 = 'à|á|ả|ã|ạ|ầ|ấ|ẩ|ẫ|ậ|ằ|ắ|ẳ|ẵ|ặ|è|é|ẻ|ẽ|ẹ|ề|ế|ể|ễ|ệ|ì|í|ỉ|ĩ|ị|ò|ó|ỏ|õ|ọ|ồ|ố|ổ|ỗ|ộ|ờ|ớ|ở|ỡ|ợ|ù|ú|ủ|ũ|ụ|ừ|ứ|ử|ữ|ự|ỳ|ý|ỷ|ỹ|ỵ|À|Á|Ả|Ã|Ạ|Ầ|Ấ|Ẩ|Ẫ|Ậ|Ằ|Ắ|Ẳ|Ẵ|Ặ|È|É|Ẻ|Ẽ|Ẹ|Ề|Ế|Ể|Ễ|Ệ|Ì|Í|Ỉ|Ĩ|Ị|Ò|Ó|Ỏ|Õ|Ọ|Ồ|Ố|Ổ|Ỗ|Ộ|Ờ|Ớ|Ở|Ỡ|Ợ|Ù|Ú|Ủ|Ũ|Ụ|Ừ|Ứ|Ử|Ữ|Ự|Ỳ|Ý|Ỷ|Ỹ|Ỵ'.split(\n",
    "        '|')\n",
    "    charutf8 = \"à|á|ả|ã|ạ|ầ|ấ|ẩ|ẫ|ậ|ằ|ắ|ẳ|ẵ|ặ|è|é|ẻ|ẽ|ẹ|ề|ế|ể|ễ|ệ|ì|í|ỉ|ĩ|ị|ò|ó|ỏ|õ|ọ|ồ|ố|ổ|ỗ|ộ|ờ|ớ|ở|ỡ|ợ|ù|ú|ủ|ũ|ụ|ừ|ứ|ử|ữ|ự|ỳ|ý|ỷ|ỹ|ỵ|À|Á|Ả|Ã|Ạ|Ầ|Ấ|Ẩ|Ẫ|Ậ|Ằ|Ắ|Ẳ|Ẵ|Ặ|È|É|Ẻ|Ẽ|Ẹ|Ề|Ế|Ể|Ễ|Ệ|Ì|Í|Ỉ|Ĩ|Ị|Ò|Ó|Ỏ|Õ|Ọ|Ồ|Ố|Ổ|Ỗ|Ộ|Ờ|Ớ|Ở|Ỡ|Ợ|Ù|Ú|Ủ|Ũ|Ụ|Ừ|Ứ|Ử|Ữ|Ự|Ỳ|Ý|Ỷ|Ỹ|Ỵ\".split(\n",
    "        '|')\n",
    "    for i in range(len(char1252)):\n",
    "        dic[char1252[i]] = charutf8[i]\n",
    "    return dic\n",
    "\n",
    "\n",
    "dicchar = loaddicchar()\n",
    "\n",
    "\n",
    "def convert_unicode(txt):\n",
    "    return re.sub(\n",
    "        r'à|á|ả|ã|ạ|ầ|ấ|ẩ|ẫ|ậ|ằ|ắ|ẳ|ẵ|ặ|è|é|ẻ|ẽ|ẹ|ề|ế|ể|ễ|ệ|ì|í|ỉ|ĩ|ị|ò|ó|ỏ|õ|ọ|ồ|ố|ổ|ỗ|ộ|ờ|ớ|ở|ỡ|ợ|ù|ú|ủ|ũ|ụ|ừ|ứ|ử|ữ|ự|ỳ|ý|ỷ|ỹ|ỵ|À|Á|Ả|Ã|Ạ|Ầ|Ấ|Ẩ|Ẫ|Ậ|Ằ|Ắ|Ẳ|Ẵ|Ặ|È|É|Ẻ|Ẽ|Ẹ|Ề|Ế|Ể|Ễ|Ệ|Ì|Í|Ỉ|Ĩ|Ị|Ò|Ó|Ỏ|Õ|Ọ|Ồ|Ố|Ổ|Ỗ|Ộ|Ờ|Ớ|Ở|Ỡ|Ợ|Ù|Ú|Ủ|Ũ|Ụ|Ừ|Ứ|Ử|Ữ|Ự|Ỳ|Ý|Ỷ|Ỹ|Ỵ',\n",
    "        lambda x: dicchar[x.group()], txt\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "bang_nguyen_am = [['a', 'à', 'á', 'ả', 'ã', 'ạ', 'a'],\n",
    "                  ['ă', 'ằ', 'ắ', 'ẳ', 'ẵ', 'ặ', 'aw'],\n",
    "                  ['â', 'ầ', 'ấ', 'ẩ', 'ẫ', 'ậ', 'aa'],\n",
    "                  ['e', 'è', 'é', 'ẻ', 'ẽ', 'ẹ', 'e'],\n",
    "                  ['ê', 'ề', 'ế', 'ể', 'ễ', 'ệ', 'ee'],\n",
    "                  ['i', 'ì', 'í', 'ỉ', 'ĩ', 'ị', 'i'],\n",
    "                  ['o', 'ò', 'ó', 'ỏ', 'õ', 'ọ', 'o'],\n",
    "                  ['ô', 'ồ', 'ố', 'ổ', 'ỗ', 'ộ', 'oo'],\n",
    "                  ['ơ', 'ờ', 'ớ', 'ở', 'ỡ', 'ợ', 'ow'],\n",
    "                  ['u', 'ù', 'ú', 'ủ', 'ũ', 'ụ', 'u'],\n",
    "                  ['ư', 'ừ', 'ứ', 'ử', 'ữ', 'ự', 'uw'],\n",
    "                  ['y', 'ỳ', 'ý', 'ỷ', 'ỹ', 'ỵ', 'y']]\n",
    "bang_ky_tu_dau = ['', 'f', 's', 'r', 'x', 'j']\n",
    "\n",
    "nguyen_am_to_ids = {}\n",
    "\n",
    "for i in range(len(bang_nguyen_am)):\n",
    "    for j in range(len(bang_nguyen_am[i]) - 1):\n",
    "        nguyen_am_to_ids[bang_nguyen_am[i][j]] = (i, j)\n",
    "\n",
    "def chuan_hoa_dau_tu_tieng_viet(word):\n",
    "    if not is_valid_vietnam_word(word):\n",
    "        return word\n",
    "\n",
    "    chars = list(word)\n",
    "    dau_cau = 0\n",
    "    nguyen_am_index = []\n",
    "    qu_or_gi = False\n",
    "    for index, char in enumerate(chars):\n",
    "        x, y = nguyen_am_to_ids.get(char, (-1, -1))\n",
    "        if x == -1:\n",
    "            continue\n",
    "        elif x == 9:  # check qu\n",
    "            if index != 0 and chars[index - 1] == 'q':\n",
    "                chars[index] = 'u'\n",
    "                qu_or_gi = True\n",
    "        elif x == 5:  # check gi\n",
    "            if index != 0 and chars[index - 1] == 'g':\n",
    "                chars[index] = 'i'\n",
    "                qu_or_gi = True\n",
    "        if y != 0:\n",
    "            dau_cau = y\n",
    "            chars[index] = bang_nguyen_am[x][0]\n",
    "        if not qu_or_gi or index != 1:\n",
    "            nguyen_am_index.append(index)\n",
    "    if len(nguyen_am_index) < 2:\n",
    "        if qu_or_gi:\n",
    "            if len(chars) == 2:\n",
    "                x, y = nguyen_am_to_ids.get(chars[1])\n",
    "                chars[1] = bang_nguyen_am[x][dau_cau]\n",
    "            else:\n",
    "                x, y = nguyen_am_to_ids.get(chars[2], (-1, -1))\n",
    "                if x != -1:\n",
    "                    chars[2] = bang_nguyen_am[x][dau_cau]\n",
    "                else:\n",
    "                    chars[1] = bang_nguyen_am[5][dau_cau] if chars[1] == 'i' else bang_nguyen_am[9][dau_cau]\n",
    "            return ''.join(chars)\n",
    "        return word\n",
    "\n",
    "    for index in nguyen_am_index:\n",
    "        x, y = nguyen_am_to_ids[chars[index]]\n",
    "        if x == 4 or x == 8:  # ê, ơ\n",
    "            chars[index] = bang_nguyen_am[x][dau_cau]\n",
    "            # for index2 in nguyen_am_index:\n",
    "            #     if index2 != index:\n",
    "            #         x, y = nguyen_am_to_ids[chars[index]]\n",
    "            #         chars[index2] = bang_nguyen_am[x][0]\n",
    "            return ''.join(chars)\n",
    "\n",
    "    if len(nguyen_am_index) == 2:\n",
    "        if nguyen_am_index[-1] == len(chars) - 1:\n",
    "            x, y = nguyen_am_to_ids[chars[nguyen_am_index[0]]]\n",
    "            chars[nguyen_am_index[0]] = bang_nguyen_am[x][dau_cau]\n",
    "            # x, y = nguyen_am_to_ids[chars[nguyen_am_index[1]]]\n",
    "            # chars[nguyen_am_index[1]] = bang_nguyen_am[x][0]\n",
    "        else:\n",
    "            # x, y = nguyen_am_to_ids[chars[nguyen_am_index[0]]]\n",
    "            # chars[nguyen_am_index[0]] = bang_nguyen_am[x][0]\n",
    "            x, y = nguyen_am_to_ids[chars[nguyen_am_index[1]]]\n",
    "            chars[nguyen_am_index[1]] = bang_nguyen_am[x][dau_cau]\n",
    "    else:\n",
    "        # x, y = nguyen_am_to_ids[chars[nguyen_am_index[0]]]\n",
    "        # chars[nguyen_am_index[0]] = bang_nguyen_am[x][0]\n",
    "        x, y = nguyen_am_to_ids[chars[nguyen_am_index[1]]]\n",
    "        chars[nguyen_am_index[1]] = bang_nguyen_am[x][dau_cau]\n",
    "        # x, y = nguyen_am_to_ids[chars[nguyen_am_index[2]]]\n",
    "        # chars[nguyen_am_index[2]] = bang_nguyen_am[x][0]\n",
    "    return ''.join(chars)\n",
    "\n",
    "\n",
    "def is_valid_vietnam_word(word):\n",
    "    chars = list(word)\n",
    "    nguyen_am_index = -1\n",
    "    for index, char in enumerate(chars):\n",
    "        x, y = nguyen_am_to_ids.get(char, (-1, -1))\n",
    "        if x != -1:\n",
    "            if nguyen_am_index == -1:\n",
    "                nguyen_am_index = index\n",
    "            else:\n",
    "                if index - nguyen_am_index != 1:\n",
    "                    return False\n",
    "                nguyen_am_index = index\n",
    "    return True\n",
    "\n",
    "\n",
    "def chuan_hoa_dau_cau_tieng_viet(sentence):\n",
    "    \"\"\"\n",
    "        Chuyển câu tiếng việt về chuẩn gõ dấu kiểu cũ.\n",
    "        :param sentence:\n",
    "        :return:\n",
    "        \"\"\"\n",
    "    words = sentence.split()\n",
    "    for index, word in enumerate(words):\n",
    "        cw = re.sub(r'(^\\p{P}*)([p{L}.]*\\p{L}+)(\\p{P}*$)', r'\\1/\\2/\\3', word).split('/')\n",
    "        # print(cw)\n",
    "        if len(cw) == 3:\n",
    "            cw[1] = chuan_hoa_dau_tu_tieng_viet(cw[1])\n",
    "        words[index] = ''.join(cw)\n",
    "    return ' '.join(words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "sp_word_sub = {\n",
    "    \"@@\": \"confuseeyes\",\n",
    "    \"℅\": \"%\",\n",
    "    r\"/\": \" fraction \",\n",
    "    r\":\\)+\": \"smileface\",\n",
    "    r\";\\)+\": \"smileface\",\n",
    "    r\":\\*+\": \"kissingface\",\n",
    "    r\"=\\)+\": \"playfulsmileface\",\n",
    "    r\"=\\(+\": \"playfulsadface\",\n",
    "    r\":\\(+\": \"sadface\",\n",
    "    r\":3+\": \"threeface\",\n",
    "    r\":v+\": \"vface\",\n",
    "    r\"\\^\\^\": \"kindsmile\",\n",
    "    r\"\\^_\\^\": \"kindmountsmile\",\n",
    "    r\"\\^\\.\\^\": \"kindmountsmile\",\n",
    "    r\"-_-\": \"disapointface\",\n",
    "    r\"\\._\\.\": \"confusedface\",\n",
    "    r\":>+\": \"cutesmile\",\n",
    "    r\"(\\|)w(\\|)\": \"fancycryface\",\n",
    "    r\":\\|\": \"mutedface\",\n",
    "    r\":d+\": \"laughface\",\n",
    "    r\"<3\": \"loveicon\",\n",
    "    r\"\\.{2,}\": \"threedot\",\n",
    "    r\"-{1,}>{1,}\": \"arrow\",\n",
    "    r\"={1,}>{1,}\": \"arrow\",\n",
    "    r\"(\\d+)h\": r\"\\1 giờ\",\n",
    "    r\"(\\d+)'\": r\"\\1 phút\",\n",
    "    r\"(\\d+)trieu\": r\"\\1 triệu\",\n",
    "    r\"(\\d+)\\s?tr\": r\"\\1 triệu\",\n",
    "    r\"blut\\w+\": \"bluetooth\",\n",
    "    r\"(\\d+)\\s\\*\": r\"\\1 sao\"\n",
    "}\n",
    "\n",
    "replace_dict = {\n",
    "    \"/\": \"fraction\",\n",
    "    \"wf\": \"wifi\",\n",
    "    \"wifj\": \"wifi\",\n",
    "    \"wjfj\": \"wifi\",\n",
    "    \"wjfi\": \"wifi\",\n",
    "    \"wiffi\": \"wifi\",\n",
    "    \"wj\": \"wifi\",\n",
    "    \"ko\": \"không\",\n",
    "    \"k\": \"không\",\n",
    "    \"hong\": \"không\",\n",
    "    \"đc\": \"được\",\n",
    "    \"sp\": \"sản phẩm\",\n",
    "    \"fb\": \"facebook\",\n",
    "    \"ytb\": \"youtube\",\n",
    "    \"yt\": \"youtube\",\n",
    "    \"mes\": \"messenger\",\n",
    "    \"mess\": \"messenger\",\n",
    "    \"tgdđ\": \"thegioididong\",\n",
    "    \"nv\": \"nhân viên\",\n",
    "    \"ss\": \"samsung\",\n",
    "    \"ip\": \"iphone\",\n",
    "    \"appel\": \"apple\",\n",
    "    \"oke\": \"ok\",\n",
    "    \"okie\": \"ok\",\n",
    "    \"okey\": \"ok\",\n",
    "    \"oki\": \"ok\",\n",
    "    \"oce\": \"ok\",\n",
    "    \"okela\": \"ok\",\n",
    "    \"mk\": \"mình\",\n",
    "    \"sd\": \"sử dụng\",\n",
    "    \"sdung\": \"sử dụng\",\n",
    "    \"ae\": \"anh em\",\n",
    "    \"lq\": \"liên quân\",\n",
    "    \"lqmb\": \"liên quân mobile\",\n",
    "    \"lun\": \"luôn\",\n",
    "    \"ng\": \"người\",\n",
    "    \"ad\": \"admin\",\n",
    "    \"ms\": \"mới\",\n",
    "    \"cx\": \"cũng\",\n",
    "    \"cũg\": \"cũng\",\n",
    "    \"nhìu\": \"nhiều\",\n",
    "    \"bth\": \"bình thường\",\n",
    "    \"bthg\": \"bình thường\",\n",
    "    \"ngta\": \"người ta\",\n",
    "    \"dow\": \"download\",\n",
    "    \"hdh\": \"hệ điều hành\",\n",
    "    \"hđh\": \"hệ điều hành\",\n",
    "    \"cammera\": \"camera\",\n",
    "    \"dt\": \"điện thoại\",\n",
    "    \"dthoai\": \"điện thoại\",\n",
    "    \"dth\": \"điện thoại\",\n",
    "    \"đth\": \"điện thoại\",\n",
    "    \"hk\": \"không\",\n",
    "    \"j\": \"gì\",\n",
    "    \"ji\": \"gì\",\n",
    "    \"mn\": \"mọi người\",\n",
    "    \"m.n\": \"mọi người\",\n",
    "    \"mjh\": \"mình\",\n",
    "    \"mjk\": \"mình\",\n",
    "    \"lắc\": \"lag\",\n",
    "    \"lác\": \"lag\",\n",
    "    \"lang\": \"lag\",\n",
    "    \"nhah\": \"nhanh\",\n",
    "    \"nóichung\": \"nói chung\",\n",
    "    \"zl\": \"zalo\",\n",
    "    \"sóg\": \"sóng\",\n",
    "    \"rẽ\": \"rẻ\",\n",
    "    \"trc\": \"trước\",\n",
    "    \"chíp\": \"chip\",\n",
    "    \"bin\": \"pin\",\n",
    "    \"lm\": \"làm\",\n",
    "    \"bik\": \"biết\",\n",
    "    \"hog\": \"không\",\n",
    "    \"zỏm\": \"dổm\",\n",
    "    \"z\": \"vậy\",\n",
    "    \"v\": \"vậy\",\n",
    "    \"nhah\": \"nhanh\",\n",
    "    \"r\": \"rồi\",\n",
    "    \"ỗn\": \"ổn\",\n",
    "    \"nhìu\": \"nhiều\",\n",
    "    \"wá\": \"quá\",\n",
    "    \"wep\": \"web\",\n",
    "    \"wed\": \"web\",\n",
    "    \"fim\": \"phim\",\n",
    "    \"film\": \"phim\",\n",
    "    \"xạc\": \"sạc\",\n",
    "    \"xài\": \"sài\",\n",
    "    \"het\": \"hết\",\n",
    "    \"lun\": \"luôn\",\n",
    "    \"e\": \"em\",\n",
    "    \"a\": \"anh\",\n",
    "    \"bjo\": \"bây giờ\",\n",
    "    \"vl\": \"vãi lồn\",\n",
    "    \"sac\": \"sạc\",\n",
    "    \"vidieo\": \"video\",\n",
    "    \"tét\": \"test\",\n",
    "    \"tes\": \"test\",\n",
    "    \"thik\": \"thích\",\n",
    "    \"fai\": \"phải\",\n",
    "    \"✋\": \"tay\",\n",
    "    \"🔋\": \"pin\",\n",
    "    \"☆\": \"sao\",\n",
    "    \"supper\": \"super\",\n",
    "    \"lổi\": \"lỗi\",\n",
    "    \"loát\": \"load\",\n",
    "    \"thui\": \"thôi\",\n",
    "    \"rùi\": \"rồi\",\n",
    "    \"ỗn\": \"ổn\",\n",
    "    \"lổi\": \"lỗi\",\n",
    "    \"suống\": \"xuống\",\n",
    "    \"selfi\": \"selfie\",\n",
    "    \"gg\": \"google\",\n",
    "    \"cam on\": \"cảm ơn\",\n",
    "    \"tg\": \"thời gian\",\n",
    "    \"nchung\": \"nói chung\",\n",
    "    \"❤\": \"loveicon\",\n",
    "    \"trại nghiệm\": \"trải nghiệm\",\n",
    "    \"dất\": \"rất\",\n",
    "    \"đứg\": \"đứng\",\n",
    "    \"bằg\": \"bằng\",\n",
    "    \"mìh\": \"mình\",\n",
    "    \"đag\": \"đang\",\n",
    "    \"thoi\": \"thôi\",\n",
    "    \"củng\": \"cũng\",\n",
    "    \"đả\": \"đã\",\n",
    "    \"màng\": \"màn\",\n",
    "    \"ff\": \"free fire\",\n",
    "    \"cod\": \"call of duty\",\n",
    "    \"moi thứ\": \"mọi thứ\",\n",
    "    \"moi thu\": \"mọi thứ\",\n",
    "    \"moi thư\": \"mọi thứ\",\n",
    "    \"moi người\": \"mọi người\",\n",
    "    \"moi\": \"mới\",\n",
    "    \"dk\": \"được\",\n",
    "    \"đk\": \"được\",\n",
    "    \"nhậy\": \"nhạy\",\n",
    "    \"ak\": \"á\",\n",
    "    \"ghe\": \"nghe\",\n",
    "    \"bùn\": \"buồn\",\n",
    "    \"bit\": \"biết\",\n",
    "    \"bít\": \"biết\",\n",
    "    \"bnhieu\": \"bao nhiêu\",\n",
    "    \"dụg\": \"dụng\",\n",
    "    \"tk\": \"tài khoản\",\n",
    "    \"sąc\": \"sạc\",\n",
    "    \"rât\": \"rât\",\n",
    "    \"haz\": \"haiz\",\n",
    "    \"sai làm\": \"sai lầm\",\n",
    "    \"flim\": \"phim\",\n",
    "    \"xướt\": \"xước\",\n",
    "    \"viềng\": \"viền\"\n",
    "}\n",
    "\n",
    "drop_indices = [1537, 2754, 4267, 6536]\n",
    "\n",
    "def normalize(text: str, track_change=False):\n",
    "    # Lowercase\n",
    "    text = text.lower()\n",
    "\n",
    "    text = re.sub(r\"((https?|ftp|file):\\/{2,3})+([-\\w+&@#/%=~|$?!:,.]*)|(www.)+([-\\w+&@#/%=~|$?!:,.]*)\", \"urllink\", text)\n",
    "\n",
    "    # Remove dup trailing chars (troiiiii -> troi)\n",
    "    text = re.sub(r\"([\\D\\w])\\1+\\b\", r\"\\1\", text)\n",
    "    if track_change:\n",
    "        print(\"Dedup trailing: \", text)\n",
    "\n",
    "    # Replace special symbol to word\n",
    "    for pttn, repl in sp_word_sub.items():\n",
    "        text = re.sub(fr\"{pttn}\", f\" {repl} \", text)\n",
    "    if track_change:\n",
    "        print(\"Replace special word: \", text)\n",
    "    \n",
    "    # Correct misspelled word\n",
    "    def replace(match):\n",
    "        orig = match.group(1)\n",
    "        word = \" \" + replace_dict.get(orig, orig) + \" \"\n",
    "        return word\n",
    "    text = re.sub(r\"\\b(\\S+)\\b\", replace, text)\n",
    "    if track_change:\n",
    "        print(\"Correct misspelled word: \", text)\n",
    "\n",
    "    # Normalize string encoding\n",
    "    text = convert_unicode(text)\n",
    "    if track_change:\n",
    "        print(\"Normalize string encoding: \", text)\n",
    "\n",
    "    # Vietnamese unicode normalization\n",
    "    text = chuan_hoa_dau_cau_tieng_viet(text)\n",
    "    if track_change:\n",
    "        print(\"Vietnamese unicode normalization: \", text)\n",
    "\n",
    "    # Eliminate decimal delimiter (9.000 -> 9000)\n",
    "    text = re.sub(r\"(?<=\\d)\\.(?=\\d{3})\", \"\", text)\n",
    "    if track_change:\n",
    "        print(\"Eliminate decimal delimiter: \", text)\n",
    "    \n",
    "    # Split between value and unit (300km -> 300 km)\n",
    "    text = re.sub(r\"(\\d+)(\\D+)\", r\"\\1 \\2\", text)\n",
    "    if track_change:\n",
    "        print(\"Split between value and unit: \", text)\n",
    "\n",
    "    # Split by punctuations\n",
    "    text = \" \".join(\n",
    "        re.split(\"([\"+re.escape(\"!\\\"#$%&\\'()*+,-./:;<=>?@[\\\\]^`{|}~\")+\"])\", text)\n",
    "    )\n",
    "    if track_change:\n",
    "        print(\"Split by punctuations: \", text)\n",
    "\n",
    "    # Split by emoticons\n",
    "    text = \" \".join(\n",
    "        re.split(\"([\"\n",
    "            u\"\\U0001F600-\\U0001F64F\"  # emoticons\n",
    "            u\"\\U0001F300-\\U0001F5FF\"  # symbols & pictographs\n",
    "            u\"\\U0001F680-\\U0001F6FF\"  # transport & map symbols\n",
    "            u\"\\U0001F1E0-\\U0001F1FF\"  # flags (iOS)\n",
    "            u\"\\U00002702-\\U000027B0\"\n",
    "            u\"\\U000024C2-\\U0001F251\"\n",
    "            u\"\\U0001f926-\\U0001f937\"\n",
    "            u'\\U00010000-\\U0010ffff'\n",
    "            u\"\\u200d\"\n",
    "            u\"\\u2640-\\u2642\"\n",
    "            u\"\\u2600-\\u2B55\"\n",
    "            u\"\\u23cf\"\n",
    "            u\"\\u23e9\"\n",
    "            u\"\\u231a\"\n",
    "            u\"\\u3030\"\n",
    "            u\"\\ufe0f\"\n",
    "            u\"\\u221a\"\n",
    "        \"])\", text)\n",
    "    )\n",
    "\n",
    "    # Word segmentation\n",
    "    # text = \" \".join(vncorenlp.word_segment(text))\n",
    "    \n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dedup trailing:  ủa cho em hỏi là iphone 7plus chính hãng của tgd là không có loại 64g hay 128g dạ ? hay là nó độc quyền 32g à . còn mấy cái 7plus g cao là hàng nhái hay sao ? tại em ra tgd kiếm mà chỉ có 32g thôi nên mua luôn 😂 giờ thắc mắc lên hỏi 🤔🤔\n",
      "Replace special word:  ủa cho em hỏi là iphone 7plus chính hãng của tgd là không có loại 64g hay 128g dạ ? hay là nó độc quyền 32g à . còn mấy cái 7plus g cao là hàng nhái hay sao ? tại em ra tgd kiếm mà chỉ có 32g thôi nên mua luôn 😂 giờ thắc mắc lên hỏi 🤔🤔\n",
      "Correct misspelled word:   ủa   cho   em   hỏi   là   iphone   7plus   chính   hãng   của   tgd   là   không   có   loại   64g   hay   128g   dạ  ?  hay   là   nó   độc   quyền   32g   à  .  còn   mấy   cái   7plus   g   cao   là   hàng   nhái   hay   sao  ?  tại   em   ra   tgd   kiếm   mà   chỉ   có   32g   thôi   nên   mua   luôn  😂  giờ   thắc   mắc   lên   hỏi  🤔🤔\n",
      "Normalize string encoding:   ủa   cho   em   hỏi   là   iphone   7plus   chính   hãng   của   tgd   là   không   có   loại   64g   hay   128g   dạ  ?  hay   là   nó   độc   quyền   32g   à  .  còn   mấy   cái   7plus   g   cao   là   hàng   nhái   hay   sao  ?  tại   em   ra   tgd   kiếm   mà   chỉ   có   32g   thôi   nên   mua   luôn  😂  giờ   thắc   mắc   lên   hỏi  🤔🤔\n",
      "Vietnamese unicode normalization:  ủa cho em hỏi là iphone 7plus chính hãng của tgd là không có loại 64g hay 128g dạ ? hay là nó độc quyền 32g à . còn mấy cái 7plus g cao là hàng nhái hay sao ? tại em ra tgd kiếm mà chỉ có 32g thôi nên mua luôn 😂 giờ thắc mắc lên hỏi 🤔🤔\n",
      "Eliminate decimal delimiter:  ủa cho em hỏi là iphone 7plus chính hãng của tgd là không có loại 64g hay 128g dạ ? hay là nó độc quyền 32g à . còn mấy cái 7plus g cao là hàng nhái hay sao ? tại em ra tgd kiếm mà chỉ có 32g thôi nên mua luôn 😂 giờ thắc mắc lên hỏi 🤔🤔\n",
      "Split between value and unit:  ủa cho em hỏi là iphone 7 plus chính hãng của tgd là không có loại 64 g hay 128 g dạ ? hay là nó độc quyền 32 g à . còn mấy cái 7 plus g cao là hàng nhái hay sao ? tại em ra tgd kiếm mà chỉ có 32 g thôi nên mua luôn 😂 giờ thắc mắc lên hỏi 🤔🤔\n",
      "Split by punctuations:  ủa cho em hỏi là iphone 7 plus chính hãng của tgd là không có loại 64 g hay 128 g dạ  ?  hay là nó độc quyền 32 g à  .  còn mấy cái 7 plus g cao là hàng nhái hay sao  ?  tại em ra tgd kiếm mà chỉ có 32 g thôi nên mua luôn 😂 giờ thắc mắc lên hỏi 🤔🤔\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'ủa cho em hỏi là iphone 7 plus chính hãng của tgd là không có loại 64 g hay 128 g dạ  ?  hay là nó độc quyền 32 g à  .  còn mấy cái 7 plus g cao là hàng nhái hay sao  ?  tại em ra tgd kiếm mà chỉ có 32 g thôi nên mua luôn  😂  giờ thắc mắc lên hỏi  🤔  🤔 '"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "normalize(\"Ủa cho em hỏi là iphone 7plus chính hãng của tgdd là không có loại 64G hay 128G dạ ? Hay là nó độc quyền 32G à . Còn mấy cái 7plus G cao là hàng nhái hay sao ? Tại em ra tgdd kiếm mà chỉ có 32G thôi nên mua luôn 😂 giờ thắc mắc lên hỏi 🤔🤔\", track_change=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "\n",
    "for dset in [\"Train\", \"Dev\", \"Test\"]:\n",
    "    df = pd.read_csv(f\"data/{dset}.csv\")\n",
    "    if dset == \"Train\":\n",
    "        df.drop(index=drop_indices)\n",
    "    normalized = df.comment.map(normalize)\n",
    "    with open(f\"data/prep-{dset}.txt\", \"w\") as f:\n",
    "        f.write(\"\\n\".join(normalized))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
